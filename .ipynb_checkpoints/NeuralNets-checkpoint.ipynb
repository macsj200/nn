{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import time, json\n",
    "\n",
    "# local imports\n",
    "from activation_function import Sigmoid\n",
    "from cost_function import CrossEntropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# uncomment and run this line of code if you get an error while running mnist\n",
    "# !pip install python-mnist\n",
    "\n",
    "# uncomment to download mnist\n",
    "# !./get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "\n",
    "def load_mnist_dataset():\n",
    "    '''\n",
    "    Loads the mnist dataset from a folder\n",
    "    :return: The training and tessting sets as specified by the mnist package\n",
    "    '''\n",
    "    mndata = MNIST('./data/')\n",
    "    X_train, labels_train = map(np.array, mndata.load_training())\n",
    "    # the test labels don't mean anything with the new test data\n",
    "    X_test, _ = map(np.array, mndata.load_testing())\n",
    "    X_train = X_train / 255.0\n",
    "    return X_train, labels_train\n",
    "def mnist_classify(yhat):\n",
    "    '''\n",
    "    Takes in one-hot encoded vectors of the mnist dataset\n",
    "    and returns the most probable digit.\n",
    "    Simply returns the index of the maximum value\n",
    "    :param yhat: 10xnum_samples\n",
    "    :return: digit of highest classification probability\n",
    "    '''\n",
    "    return np.argmax(yhat, axis=1)\n",
    "def accuracy(y, yhat):\n",
    "    '''\n",
    "    Calculates the accuracy of a network prediction yhat for label values y\n",
    "    :param y:\n",
    "    :param yhat:\n",
    "    :return:\n",
    "    '''\n",
    "    return np.sum(mnist_classify(y) == mnist_classify(yhat)) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single - Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(labels_train, num_classes=10):\n",
    "    '''Convert categorical labels to standard basis vectors in R^{num_classes} '''\n",
    "    return np.eye(num_classes)[labels_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_every = 1000\n",
    "class NeuralNet:\n",
    "    '''\n",
    "    A Neural Network class.\n",
    "    Uses the logistic as all activation functions.\n",
    "    The bias terms are separate from the weight matrix.\n",
    "    '''\n",
    "    def __init__(self, sizes):\n",
    "        '''\n",
    "        Creates a neural net where the hidden layers are all activated by ReLUs and the output is activated by softmax\n",
    "        :param: neurons - the number of neurons per layer\n",
    "        e.g. [2,3,4] would indicate a network that has 3 layers with\n",
    "        2 neurons in the input, 4 on the output, and 3 in the single hidden layer\n",
    "        '''\n",
    "        assert len(sizes) >= 2, \"Sizes must include input and output sizes;len > =2\"\n",
    "        self.n_input = sizes[0]  # number of input neurons\n",
    "        self.n_output = sizes[-1]  # number of output neurons\n",
    "        self.n_layers = len(sizes) - 1\n",
    "        self.sizes = sizes\n",
    "        self.create_net()\n",
    "\n",
    "    def create_net(self):\n",
    "        '''\n",
    "        Initializes the weights and acitvation functions of the net\n",
    "        '''\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i, o in zip(self.sizes[:-1], self.sizes[1:]):\n",
    "            self.weights.append(np.random.randn(o, i ) / np.sqrt(o))\n",
    "            self.biases.append(np.random.randn(o, 1))\n",
    "        self.fn = (self.n_layers) * [Sigmoid]\n",
    "\n",
    "    def feed(self, x):\n",
    "        '''\n",
    "        Internal feed forward for SGD. \n",
    "        '''\n",
    "        x = x.T\n",
    "        self.activations = [x]\n",
    "        self.sums = [x]\n",
    "        for w, b, a in zip(self.weights, self.biases, self.fn):\n",
    "            linear_combo = w @ x + b\n",
    "            x = a.fn(linear_combo)\n",
    "            self.sums.append(linear_combo)\n",
    "            self.activations.append(x)\n",
    "        return x if 1 in x.shape else x.T\n",
    "\n",
    "    def _update_weights(self, learning_rate):\n",
    "        ''' Takes the stored deltaweights and updates all the weights.\n",
    "        Useful so backpropagation doesn't require any extra copying.'''\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] = self.weights[i] - learning_rate * self.dweights[i]\n",
    "            self.biases[i] = self.biases[i] - learning_rate * self.dbiases[i]\n",
    "\n",
    "    def train(self, X_train, y_train, epochs, X_validation=[], y_validation=[], learning_rate=1e-3):\n",
    "        '''\n",
    "        SGD training w/ mini-batch size 1\n",
    "        '''\n",
    "        training_loss, validation_loss = [], []\n",
    "        training_accuracy, validation_accuracy = [], []\n",
    "\n",
    "        for e in range(epochs):\n",
    "            X_train, y_train = shuffle(X_train, y_train)\n",
    "            # the stochastic part\n",
    "            # reshape to place the data on the rows\n",
    "            x = X_train[0].reshape((1,self.n_input))\n",
    "            y = y_train[0].reshape((self.n_output, 1))\n",
    "            yhat = self.feed(x)\n",
    "\n",
    "            delta = yhat - y\n",
    "\n",
    "            # reinitialize dweights\n",
    "            self.dweights = [np.zeros(w.shape) for w in self.weights]\n",
    "            self.dbiases = [np.zeros(b.shape) for b in self.biases]\n",
    "            self.dweights[-1] = np.outer(delta, self.activations[-2])\n",
    "            self.dweights[-1] = delta\n",
    "\n",
    "            prevWeights = self.weights[-1]\n",
    "            for l in range(2, len(self.weights) + 1):\n",
    "                dact = self.fn[-l].deriv\n",
    "                sum_inputs = self.sums[-l]\n",
    "                # remove the last row because it is the result on the bias\n",
    "                # we don't deal with that in our delta calculation\n",
    "                delta = dact(sum_inputs) * (prevWeights.T @ delta)\n",
    "                self.dbiases[-l] = delta\n",
    "                self.dweights[-l] = np.outer(delta, self.activations[-(l + 1)])\n",
    "                prevWeights = self.weights[-l]\n",
    "            self._update_weights(learning_rate)\n",
    "            if e % plot_every == 0:\n",
    "                train_feed = self.feed(X_train)\n",
    "                training_loss.append(CrossEntropy.fn(y_train, train_feed))\n",
    "                training_accuracy.append(accuracy(y_train, train_feed))\n",
    "                if len(X_validation) > 0:\n",
    "                    validation_feed = self.feed(X_validation)\n",
    "                    validation_loss.append(CrossEntropy.fn(y_validation, validation_feed))\n",
    "                    validation_accuracy.append(accuracy(y_validation, validation_feed))\n",
    "        return training_loss, validation_loss, training_accuracy, validation_accuracy\n",
    "\n",
    "    def save(self, filename='net'):\n",
    "        '''\n",
    "        Save the neural network\n",
    "        '''\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\" : [b.tolist() for b in self.biases]\n",
    "                }\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()\n",
    "        \n",
    "def load(filename):\n",
    "    '''\n",
    "    Load a neural network from the file ``filename``.  Returns an\n",
    "    instance of Network.\n",
    "    '''\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    net = NeuralNet(data[\"sizes\"])\n",
    "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_error(training_error, validation_error, title, save=False):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.plot(training_error, label=\"training\")\n",
    "    plt.xlabel('Iteration ${0}$x'.format(plot_every))\n",
    "    plt.plot(validation_error, label=\"validation\")\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading MNIST dataset\")\n",
    "features_train, labels_train = load_mnist_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Hot Encoding Labels\n"
     ]
    }
   ],
   "source": [
    "print(\"One Hot Encoding Labels\")\n",
    "ohe_labels_train = one_hot(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Validation Split\n"
     ]
    }
   ],
   "source": [
    "print(\"Train-Validation Split\")\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(features_train, ohe_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize Neural Network\n",
      "Start Training\n"
     ]
    }
   ],
   "source": [
    "print(\"Initialize Neural Network\")\n",
    "nn= NeuralNet([784,800,10])\n",
    "print(\"Start Training\")\n",
    "start = time.clock()\n",
    "training_error, validation_error, \\\n",
    "    training_accuracy, validation_accuracy = nn.train(X_train, y_train,\n",
    "                                                      epochs=10,\n",
    "                                                      X_validation=X_validation, y_validation=y_validation)\n",
    "end = time.clock()\n",
    "elapsed = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Network\n",
      "Time elapse: {0} minutes 0.21763849999999987\n",
      "Final Training accuracy 0.0568222222222\n",
      "Final Validation accuracy 0.0576666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving Network\")\n",
    "nn.save('net1')\n",
    "print(\"Time elapse: {0} minutes\", elapsed / 60)\n",
    "plot_error(training_error, validation_error, 'Cross-entropy error')\n",
    "plot_error(training_accuracy, validation_accuracy, 'Classification accuracy')\n",
    "print('Final Training accuracy', accuracy(y_train, nn.feed(X_train)))\n",
    "print('Final Validation accuracy', accuracy(y_validation, nn.feed(X_validation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn1 = load('net1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056822222222222225"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_train, nn1.feed(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phishingdf = pd.read_csv('../../datasets/phishing/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['having_IP_Address', 'URL_Length', 'Shortining_Service',\n",
       "       'having_At_Symbol', 'double_slash_redirecting', 'Prefix_Suffix',\n",
       "       'having_Sub_Domain', 'SSLfinal_State', 'Domain_registeration_length',\n",
       "       'Favicon', 'port', 'HTTPS_token', 'Request_URL', 'URL_of_Anchor',\n",
       "       'Links_in_tags', 'SFH', 'Submitting_to_email', 'Abnormal_URL',\n",
       "       'Redirect', 'on_mouseover', 'RightClick', 'popUpWidnow', 'Iframe',\n",
       "       'age_of_domain', 'DNSRecord', 'web_traffic', 'Page_Rank',\n",
       "       'Google_Index', 'Links_pointing_to_page', 'Statistical_report',\n",
       "       'Result'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phishingdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(phishingdf['age_of_domain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
